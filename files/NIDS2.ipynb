{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('kddcup.data_10_percent_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features:**\n",
    " \n",
    "  \n",
    "\n",
    "*   **duration:**\t \n",
    "    length (number of seconds) of the connection\t\t\t\t\n",
    "*   **protocol_type:**\t \t \n",
    "    type of the protocol, e.g. tcp, udp, etc.\t\t\t\t\t\n",
    "*   **service:**\t \t \n",
    "    network service on the destination, e.g., http, telnet, etc.\t\n",
    "*   **flag:**\t \t \n",
    "    normal or error status of the connection \t\n",
    "*   **src_bytes:** \t \n",
    "    number of data bytes from source to destination\t\t\t\n",
    "*   **dst_bytes:**\t\t \n",
    "    number of data bytes from destination to source\n",
    "*   **land:**\t\t \n",
    "    1 if connection is from/to the same host/port; 0 otherwise\t\n",
    "*   **wrong_fragment:**\t\t \n",
    "    number of “wrong” fragments\t\n",
    "*   **urgent:**\t\t \n",
    "    number of urgent packets\n",
    "*   **hot:**\t\t \n",
    "    number of “hot” indicators\n",
    "*   **num_failed_logins:** \t \n",
    "    number of failed login attempts\n",
    "*   **logged_in:**\t\t \n",
    "    11 if successfully logged in; 0 otherwise\t\n",
    "*   **num_compromised:**\t\t \n",
    "    number of “compromised” conditions\n",
    "*   **root_shell:**\t\t \n",
    "    1 if root shell is obtained; 0 otherwise\t\n",
    "*   **su_attempted:**\t\t \n",
    "    1 if “su root” command attempted; 0 otherwise\n",
    "*   **num_root:**\t\t \n",
    "    number of “root” accesses\n",
    "*   **num_file_creations:**\t\t \n",
    "    number of file creation operations\t\n",
    "*   **num_shells:**\t\t \n",
    "    number of shell prompts\n",
    "*   **num_access_files:**\t\t \n",
    "    number of operations on access control files\t\n",
    "*   **num_outbound_cmds:**\t\t \n",
    "    number of outbound commands in an ftp session\t\n",
    "*   **is_host_login:**\t\t \n",
    "    1 if the login belongs to the “hot” list; 0 otherwise\n",
    "*   **is_guest_login:**\t\t \n",
    "    1 if the login is a “guest”login; 0 otherwise\n",
    "*   **count:**\t\t \n",
    "    number of connections to the same host as the current connection in the past two seconds\n",
    "*   **srv_count:**\t\t \n",
    "    Number of connection to the same service (port number)\n",
    "*   **serror_rate:**\t\t \n",
    "    Percentage of connections that have activated flag (#4) s0,s1,s2 or s3, among the connections aggregated in count (#23)\n",
    "*   **srv_rerror_rate:**\t\t \n",
    "    Percentage of connection that have activated flag (#4) s0,s1,s2 or s3, among the connections aggregated in srv count (#24)\n",
    "*   **rerror_rate:**\t\t \n",
    "    Percentage of connections that have activated flag (#4 )REJ, among the connections aggregated in count (#23)\n",
    "*   **srv_rerror_rate:** \t \n",
    "    Percentage of connections that have activated flag (#4) REJ, among the connections aggregated in srv count (#24)\n",
    "*   **same_srv_rate:**\t\t \n",
    "    Percentage of connections that were to the same services, among the connections aggregated in count (#23)\n",
    "*   **diff_srv_rate:**\t\t \n",
    "    Percentage of connections that were to the different services, among the connections aggregated in count (#23)\n",
    "*   **srv_count:**\t\t \n",
    "    Number of connection to the same service (port number)\n",
    "*   **srv_diff_host_rate:**\t\t \n",
    "    Percentage of connections that were to different destination machines among the connections aggregated in srv count (#24)\n",
    "*   **dst_host_count:**\t\t \n",
    "    Number of connections having the same destination host IP address\n",
    "*   **dst_host_srv_count:**\t\t \n",
    "    Number of connections having same port number\n",
    "*   **dst_host_same_srv_rate:**\t\t \n",
    "    Percentage of connections that were to the same service among\n",
    "the connections aggregated in dst host count (#32)\n",
    "*   **dst_host_diff_srv_rate:**\t\t \n",
    "    Percentage of connections that were to different service among the connections aggregated in dst host count (#32)\n",
    "*   **dst_host_same_src_port_rate:**\t\t \n",
    "    Percentage of connections that were to the same source port among the connections aggregated in dst host srv count (#33)\n",
    "*   **dst_host_srv_diff_host_rate:**\t\t \n",
    "    Percentage of connections that were to the different destination machines among the connections aggregated in dthtt(#33)\n",
    "*   **dst_host_serror_rate:**\t\t \n",
    "    Percentage of connections that have activated flag (#4) s0,s1,s2 or s3, among the connections aggregated in dst host count (#32)\n",
    "*   **dst_host_srv_serror_rate:**\t\t \n",
    "    Percentage of connections that have activated flag (#4) REJ, among the connections aggregated in dst host count (#32)\n",
    "*   **dst_host_rerror_rate:**\t\t \n",
    "    Percentage of connections that have activated flag (#4) REJ, among the connections aggregated in dst host count (#32)\t\n",
    "*   **dst_host_srv_rerror_rate:**\t\t \n",
    "    Percentage of connections that have activated flag (#4) REJ, among the connections aggregated in dst host srv count (#32)\n",
    "*   **label:**\t\t \n",
    "    Attack class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename(columns = {'label':'outcome'}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyzing our database**\n",
    "\n",
    "To analyze our database we need to get the percentage of every possible value in each column, to do that:\n",
    "\n",
    "\n",
    "\n",
    "*   expand all categories values in each column\n",
    "*   get number of categories values in each column in the dataset\n",
    "*   get the percentage of every categories value in each column\n",
    "*   run the fonction on each column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(df):\n",
    "    print()\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,\\\n",
    "                int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "analyze(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing the dataset**\n",
    "\n",
    "To Preprocess our database we need to :\n",
    "\n",
    "*   Drop the features that are not required\n",
    "*   Separate the features and labels\n",
    "*   Encode the categorical labels into integers\n",
    "\n",
    "    > To encode the categorical labels we need LabelEncoder function that is used to normalize labels. It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. Fit label encoder. Fit label encoder and return encoded labels.\n",
    "*   One-hot encoding the labels\n",
    "\n",
    "    > One hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. One hot encoding is a crucial part of feature engineering for machine learning.\n",
    "\n",
    "*   Feature scaling the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features that are not required\n",
    "dataset = dataset.drop(dataset.columns[[0, 1, 2, 3, 6, 11, 13, 14, 15, 20, 21]], axis=1)\n",
    "\n",
    "# Separate the features and labels\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Encode the categorical labels into integers\n",
    "label_encoder_y = LabelEncoder()\n",
    "y = label_encoder_y.fit_transform(y)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "y = onehotencoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Feature scaling the input data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into train and test sets\n",
    "X_train : X irrevocable used to fit the machine learning model.\n",
    "\n",
    "X_test : X irrevocable used to evaluate the fit machine learning model.\n",
    "\n",
    "Y_train : Y irrevocable used to fit the machine learning model.\n",
    "\n",
    "Y_test : Y irrevocable used to evaluate the fit machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNN Model 1\n",
    "\n",
    "Creating a CNN model for a classification problem using Keras layers:\n",
    "\n",
    "\n",
    "1.   Creates an instance of the Sequential class, which is a Keras model.\n",
    "\n",
    "2.   Adds a **1D convolutional layer** to the model with **64 filters**, a kernel size of **3**, and a **ReLU** activation function. The input shape is specified as **(X_train.shape[1], 1)** which means that the input data is **1-dimensional** and the number of features is equal to **X_train.shape[1]**.\n",
    "\n",
    "3.   Adds a max **pooling layer** to the model with a pool size of **2**.\n",
    "\n",
    "4.   Adds another **1D convolutional layer** to the model with **32 filters**, a kernel size of **3**, and a **ReLU** activation function.\n",
    "\n",
    "5.   Adds another max **pooling layer** to the model with a pool size of **2**.\n",
    "\n",
    "6.   **Flattens** the output from the **convolutional layers** into a **1-dimensional** array.\n",
    "\n",
    "7.   Adds a **fully connected layer** to the model with **128 neurons** and a **ReLU** activation function.\n",
    "\n",
    "8.   Adds a **dropout layer** to the model with a dropout rate of **0.2**. Dropout is a regularization technique that randomly drops out **(sets to zero)** some of the neurons in the layer during training, **which helps prevent overfitting**.\n",
    "\n",
    "9.   Adds the **output layer** to the model. The number of neurons in the output layer is **equal** to the number of classes in the problem, which is **y_train.shape[1]**. The activation function used is **softmax**, which produces a probability distribution over the classes.\n",
    "\n",
    "10.   Compiles the model by specifying the optimizer **(adam)**, loss function **(categorical cross-entropy)**, and evaluation metric **(accuracy)**.\n",
    "\n",
    "11.   Prints a summary of the model architecture, including the layers, number of parameters, and output shapes.\n",
    "\n",
    "    > Creates a CNN model with two 1D convolutional layers, two max pooling layers, a fully connected layer, a dropout layer, and an output layer. The model is compiled with the Adam optimizer and categorical cross-entropy loss function, and is evaluated using the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Add convolutional layer\n",
    "classifier.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Add max pooling layer\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Add another convolutional layer\n",
    "classifier.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add max pooling layer\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output from convolutional layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Add fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Add dropout layer to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "\n",
    "# Add output layer\n",
    "classifier.add(Dense(units=y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "classifier.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNN Model 2\n",
    "\n",
    "**Model 2:** Change activation function to LeakyReLU, optimizer to RMSprop, loss function to mean squared error, and increase the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Model 2\n",
    "classifier2 = Sequential()\n",
    "\n",
    "classifier2.add(Conv1D(filters=128, kernel_size=3, activation=LeakyReLU(alpha=0.2), input_shape=(X_train.shape[1], 1)))\n",
    "classifier2.add(MaxPooling1D(pool_size=2))\n",
    "classifier2.add(Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(alpha=0.2)))\n",
    "classifier2.add(MaxPooling1D(pool_size=2))\n",
    "classifier2.add(Flatten())\n",
    "classifier2.add(Dense(units=256, activation=LeakyReLU(alpha=0.2)))\n",
    "classifier2.add(Dropout(rate=0.4))\n",
    "classifier2.add(Dense(units=y_train.shape[1], activation='softmax'))\n",
    "\n",
    "classifier2.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "classifier2.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ceating CNN Model 3**\n",
    "**Model 3:** Change activation function to tanh, optimizer to SGD, loss function to binary cross-entropy, and decrease the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Model 3\n",
    "classifier3 = Sequential()\n",
    "\n",
    "classifier3.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(X_train.shape[1], 1)))\n",
    "classifier3.add(MaxPooling1D(pool_size=2))\n",
    "classifier3.add(Conv1D(filters=16, kernel_size=3, activation='tanh'))\n",
    "classifier3.add(MaxPooling1D(pool_size=2))\n",
    "classifier3.add(Flatten())\n",
    "classifier3.add(Dense(units=64, activation='tanh'))\n",
    "classifier3.add(Dropout(rate=0.1))\n",
    "classifier3.add(Dense(units=y_train.shape[1], activation='softmax'))\n",
    "\n",
    "classifier3.compile(optimizer=SGD(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "classifier3.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, I changed the activation function to LeakyReLU and tanh, optimizer to RMSprop and SGD, loss function to mean squared error and binary cross-entropy, and adjusted the dropout rate. These changes can influence the model's learning behavior and potentially impact accuracy. Remember to select appropriate activation functions, optimizers, and loss functions based on your specific task and dataset characteristics.\n",
    "\n",
    "Feel free to modify other parameters or explore different combinations to further investigate their effects on model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Train a machine learning model using a deep learning neural network called \"classifier\" on some input data **X_train** with corresponding output data **y_train**. \n",
    "\n",
    "The purpose of the neural network is to predict the output values **(y)** for new input values **(X)** that it has not seen before.\n",
    "\n",
    "The **\"fit\"** method of the **\"classifier\"** object is used to train the neural network.\n",
    "\n",
    "The **\"fit\"** method takes in the input data and output data along with some other parameters:\n",
    "\n",
    " \n",
    "\n",
    "*   **X_train.reshape(X_train.shape[0], X_train.shape[1], 1)** - This reshapes the input data to a 3D array. The first dimension is the number of training examples, the second dimension is the length of each input sequence, and the third dimension is the number of features (in this case, there is only one feature). This is necessary for the input shape of the neural network.\n",
    "\n",
    "*   **y_train** - This is the output data that corresponds to the input data.\n",
    "\n",
    "*   **epochs=10** - This specifies the number of times the entire dataset will be passed through the neural network during training.\n",
    "\n",
    "*   **batch_size=128** - This specifies the number of samples that will be used in each training iteration. In this case, the training data will be divided into batches of 128 samples and the neural network will be trained on each batch.\n",
    "\n",
    "*   **validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test)** - This specifies the validation data, which is used to evaluate the performance of the neural network during training. The validation data is also reshaped to a 3D array.\n",
    "\n",
    ">The output of the **\"fit\"** method is stored in the **\"history\"** variable. This object contains information about the performance of the neural network during training, such as the training and validation loss and accuracy for each epoch.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=10, batch_size=128, validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = classifier2.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=10, batch_size=128, validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = classifier3.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=10, batch_size=128, validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "The \"evaluate\" method takes the following parameters:\n",
    "\n",
    "*  **X_test.reshape((X_test.shape[0], X_test.shape[1], 1))** - This reshapes the test input data to the same 3D array format used during the training. The first dimension is the number of test examples, the second dimension is the length of each input sequence, and the third dimension is the number of features (in this case, there is only one feature).\n",
    "\n",
    "*  **y_test** - This is the corresponding output data for the test input data.\n",
    "\n",
    "*  **verbose=0** - This specifies the level of verbosity of the evaluation process. A value of 0 means that no progress messages will be displayed during the evaluation.\n",
    "\n",
    ">The output of the \"evaluate\" method is stored in the \"score\" variable. The \"score\" variable contains a scalar value that represents the overall performance of the model on the test data. The specific metric used for evaluation depends on the type of model and the problem being solved. For example, if the model is a classifier, the metric might be accuracy, precision, or recall. If the model is a regressor, the metric might be mean squared error or mean absolute error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model\n",
    "score = classifier.evaluate(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the models\n",
    "score1 = classifier.evaluate(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), y_test, verbose=0)\n",
    "print('Model 1 - Test loss:', score1[0])\n",
    "print('Model 1 - Test accuracy:', score1[1])\n",
    "\n",
    "score2 = classifier2.evaluate(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), y_test, verbose=0)\n",
    "print('Model 2 - Test loss:', score2[0])\n",
    "print('Model 2 - Test accuracy:', score2[1])\n",
    "\n",
    "score3 = classifier3.evaluate(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), y_test, verbose=0)\n",
    "print('Model 3 - Test loss:', score3[0])\n",
    "print('Model 3 - Test accuracy:', score3[1])\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history3.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.plot(history3.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Model 1 Train', 'Model 2 Train', 'Model 3 Train', 'Model 1 Test', 'Model 2 Test', 'Model 3 Test'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Model 1 Train', 'Model 2 Train', 'Model 3 Train', 'Model 1 Test', 'Model 2 Test', 'Model 3 Test'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('the_best_cnn_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST THE CNN1 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "database2 = {\n",
    "    'duration': [0],  # Placeholder value or actual value for duration\n",
    "    'protocol_type': ['tcp'],  # Placeholder value or actual value for protocol_type\n",
    "    'service': ['http'],  # Placeholder value or actual value for service\n",
    "    'flag': ['SF'],  # Placeholder value or actual value for flag\n",
    "    'src_bytes': [943],\n",
    "    'dst_bytes': [0],\n",
    "    'land': [0],  # Placeholder value or actual value for land\n",
    "    'wrong_fragment': [0],\n",
    "    'urgent': [0],\n",
    "    'hot': [0],\n",
    "    'num_failed_logins': [0],\n",
    "    'logged_in': [0],  # Placeholder value or actual value for logged_in\n",
    "    'num_compromised': [0],\n",
    "    'root_shell': [0],  # Placeholder value or actual value for root_shell\n",
    "    'su_attempted': [0],  # Placeholder value or actual value for su_attempted\n",
    "    'num_root': [0],  # Placeholder value or actual value for num_root\n",
    "    'num_file_creations': [0],\n",
    "    'num_shells': [0],\n",
    "    'num_access_files': [0],\n",
    "    'num_outbound_cmds': [0],\n",
    "    'is_host_login': [0],  # Placeholder value or actual value for is_host_login\n",
    "    'is_guest_login': [0],  # Placeholder value or actual value for is_guest_login\n",
    "    'count': [87],\n",
    "    'srv_count': [87],\n",
    "    'serror_rate': [1.0],\n",
    "    'srv_serror_rate': [1.0],\n",
    "    'rerror_rate': [0.0],\n",
    "    'srv_rerror_rate': [0.0],\n",
    "    'same_srv_rate': [1.0],\n",
    "    'diff_srv_rate': [0.0],\n",
    "    'srv_diff_host_rate': [0.0],\n",
    "    'dst_host_count': [255],\n",
    "    'dst_host_srv_count': [255],\n",
    "    'dst_host_same_srv_rate': [1.0],\n",
    "    'dst_host_diff_srv_rate': [0.0],\n",
    "    'dst_host_same_src_port_rate': [0.0],\n",
    "    'dst_host_srv_diff_host_rate': [0.0],\n",
    "    'dst_host_serror_rate': [1.0],\n",
    "    'dst_host_srv_serror_rate': [1.0],\n",
    "    'dst_host_rerror_rate': [0.0],\n",
    "    'dst_host_srv_rerror_rate': [0.0],\n",
    "    'outcome': ['smurf.']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(database2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(database2)\n",
    "# Drop the features that are not required\n",
    "df = df.drop(df.columns[[0, 1, 2, 3, 6, 11, 13, 14, 15, 20, 21]], axis=1)\n",
    "\n",
    "# Separate the features and labels\n",
    "X2 = df.iloc[:, :-1].values\n",
    "y2 = df.iloc[:, -1].values\n",
    "\n",
    "# Encode the categorical labels into integers\n",
    "label_encoder_y2 = LabelEncoder()\n",
    "y2 = label_encoder_y2.fit_transform(y2)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "y2 = onehotencoder.fit_transform(y2.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Feature scaling the input data\n",
    "scaler = StandardScaler()\n",
    "X2 = scaler.fit_transform(X2)\n",
    "\n",
    "# Reshape the modified example to match the original input shape\n",
    "X2_reshaped = X2.reshape((1, 30, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the modified example\n",
    "predictions = classifier.predict(X2_reshaped)\n",
    "\n",
    "# Decode the one-hot encoded predictions\n",
    "# Reshape the predictions array\n",
    "predictions_reshaped = predictions.argmax(axis=1).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "decoded_predictions = label_encoder_y2.inverse_transform(onehotencoder.inverse_transform(predictions_reshaped))\n",
    "\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", decoded_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
